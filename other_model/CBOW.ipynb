{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import random\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import copy\n",
    "\n",
    "# Turn testing to False while training your model.\n",
    "# Please, turn it back to True while submitting your code.\n",
    "_TESTING = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,4,5,6,7,8,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 10417617 \n",
    "_eps = 1e-5 # useful to avoid overflow while using cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#line_li = []\n",
    "#word_li = []\n",
    "#line = f.readline()\n",
    "#while line:\n",
    "#    line_li.append([line.strip()])\n",
    "#    word_li.extend(line.strip().split(' '))\n",
    "#    line = f.readline()\n",
    "#line = line_li[0]\n",
    "#vocab = sorted(list(set(word_li)))\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(x_len,y_len):\n",
    "    b = np.sqrt(6.0/(x_len+y_len))\n",
    "    if _TESTING:\n",
    "        np.random.seed(seed) \n",
    "    return np.random.normal(-b, b, (x_len, y_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosineSimilarity(a, b):\n",
    "    cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW:\n",
    "    \"\"\"\n",
    "    This class is an implementation of the continous bag of words model by first principles.\n",
    "    CBOW model tries to predict the center words from surrounding words.\n",
    "    \"\"\"\n",
    "    def __init__(self, text, window, n, learning_rate=1e-4):\n",
    "        \"\"\"\n",
    "        Recommended hyperparameters.\n",
    "        Initialize self.n, self.text,self.window,self.lr,self.vocab,self.word2index, self.V and self.U\n",
    "        n = desired size of word vectors\n",
    "        window = size of window\n",
    "        vocab = vocabulary of all words in the dataset -- make sure it is sorted.\n",
    "        word2index = index for each word in the vocabulary \n",
    "        V: input vector matrix of shape [n, len(vocab)] (W)\n",
    "        U: output vector matrix of shape [len(vocab),n] (W')\n",
    "        # use weight_init to initialize weights\n",
    "        \"\"\"\n",
    "        self.text = text\n",
    "        self.temp_text = text.strip().split(' ')\n",
    "        self.vocab = sorted(list(set(text.strip().split(' '))))\n",
    "        self.n = n\n",
    "        self.window = window\n",
    "        self.lr = learning_rate\n",
    "        self.word2index = dict(zip(self.vocab, [i for i in range(len(self.vocab))]))\n",
    "        self.V = weight_init(n, len(self.vocab))\n",
    "        self.U = weight_init(len(self.vocab), n)\n",
    "        self.text_index = [self.word2index[i] for i in self.temp_text]\n",
    "        \n",
    "        self.train_left_X = []\n",
    "        self.train_right_X = []\n",
    "        self.train_Y = []\n",
    "        for i in range(self.window, len(self.vocab) - self.window):\n",
    "            self.train_left_X.append(self.temp_text[i - self.window : i])\n",
    "            self.train_right_X.append(self.temp_text[i + 1 : i + self.window + 1])\n",
    "            self.train_Y.append(self.text_index[i])\n",
    "            # print(self.temp_text[i - self.window : i], self.temp_text[i], self.temp_text[i + 1 : i + self.window + 1])\n",
    "        self.idx = [i for i in range(len(self.train_Y))]\n",
    "         \n",
    "\n",
    "    def one_hot_vector(self, index):\n",
    "        \"\"\"\n",
    "        Function for one hot vector encoding.\n",
    "        :input: a word index\n",
    "        :return: one hot encoding\n",
    "        \"\"\"\n",
    "        ret = np.zeros((1, len(self.vocab)))\n",
    "        ret[0, index] = 1\n",
    "        return ret[0]\n",
    "\n",
    "    def get_vector_representation(self, one_hot=None):\n",
    "        \"\"\"\"\n",
    "        Function for vector representation from one-hot encoding.\n",
    "        :input: one hot encoding\n",
    "        :return: word vector\n",
    "        \"\"\"\n",
    "        return (self.V.dot(np.expand_dims(one_hot, 1))).T[0]\n",
    "\n",
    "    def get_average_context(self, left_context=None, right_context=None):\n",
    "        \"\"\"\n",
    "        Function for average vector generation from surrounding context of current word.\n",
    "        :input: surrounding context (left/right)\n",
    "        :return: averaged vector representation\n",
    "        \"\"\"\n",
    "        left_context.extend(right_context)\n",
    "        all_onehot = [self.one_hot_vector(self.word2index[i]) for i in left_context]\n",
    "        return self.get_vector_representation(np.stack(all_onehot).mean(0))\n",
    "\n",
    "    def get_score(self, avg_vector=None):\n",
    "        \"\"\"\n",
    "        Function for product score given an averaged vector in current context of the center word.\n",
    "        :input: averaged vector\n",
    "        :return: product score with matrix U.\n",
    "        \"\"\"\n",
    "        return self.U.dot(avg_vector)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"\n",
    "        Function to return the softmax output of given function.\n",
    "        \"\"\"\n",
    "        exp_x = np.exp(x)\n",
    "        softmax_x = exp_x / (np.sum(exp_x) + _eps)\n",
    "        return softmax_x \n",
    "\n",
    "    def compute_cross_entropy_error(self, y, y_hat):\n",
    "        \"\"\"\n",
    "        Given one hot encoding and the output of the softmax function, this function computes the\n",
    "        cross entropy error.\n",
    "        ---------\n",
    "        :input y: one_hot encoding of the current center word\n",
    "        :input y_hat: output of the softmax function\n",
    "        :return: cross entropy error\n",
    "        \"\"\"\n",
    "        return - np.log(y_hat + _eps) * y\n",
    "\n",
    "    def compute_EH(self, e):\n",
    "        \"\"\"\n",
    "        Function to compute the value of EH, \n",
    "        the sum of the output vectors of all words in the vocabulary,\n",
    "        weighted by their prediction error.\n",
    "        Look at https://arxiv.org/pdf/1411.2738.pdf for more information.\n",
    "        ---------\n",
    "        :input: e: prediction error. \n",
    "        :return: value of EH\n",
    "        \"\"\"\n",
    "        self.eh = (self.U * np.expand_dims(e, 1)).sum(0)\n",
    "        return self.eh\n",
    "\n",
    "    def update_U(self, e, avg_vector): # avg_vector: hidden h\n",
    "        \"\"\"\n",
    "        Given the cross entropy error occured in the current sample, this function updates the U matrix.\n",
    "        :return self.U\n",
    "        \"\"\"\n",
    "        self.U = self.U - self.lr * (np.expand_dims(e, 1).dot(np.expand_dims(avg_vector, 0)))\n",
    "        return self.U\n",
    "\n",
    "    def update_V(self, e, left_context=None, right_context=None):\n",
    "        \"\"\"\n",
    "        This function updates the V matrix.\n",
    "        :return: self.V\n",
    "        NOTE: Do *not* divide by the context length (as mentioned in\n",
    "        https://arxiv.org/pdf/1411.2738.pdf) You will need to either Take an appropriate sum. \n",
    "        or\n",
    "        multiply your updates with the context length instead.\n",
    "        based on your implementation.\n",
    "        \"\"\"\n",
    "        eh = self.compute_EH(e)\n",
    "        left_context.extend(right_context)\n",
    "        all_index = [self.word2index[i] for i in left_context]\n",
    "        self.V[:, all_index] -= self.lr * len(all_index) * np.expand_dims(eh, 1)\n",
    "        return self.V\n",
    "\n",
    "    def fit(self, epoch):\n",
    "        for ep in range(epoch):\n",
    "            total_err = 0\n",
    "            random.shuffle(self.idx)\n",
    "            for i in self.idx:\n",
    "                train_left = self.train_left_X[i]\n",
    "                \n",
    "                train_right = self.train_right_X[i]\n",
    "                train_target = self.train_Y[i]\n",
    "                avg_vector = self.get_average_context(left_context=train_left, right_context=train_right)\n",
    "                score = self.get_score(avg_vector)\n",
    "                pred = self.softmax(score)\n",
    "                e = pred - self.one_hot_vector(train_target)\n",
    "                error = self.compute_cross_entropy_error(self.one_hot_vector(train_target), pred)\n",
    "                total_err += error\n",
    "                self.update_U(e, avg_vector)\n",
    "                self.update_V(e, left_context=train_left, right_context=train_right)\n",
    "            print('----- epoch: {} CE: {}------'.format(ep, total_err.sum() / len(self.idx)))\n",
    "    \n",
    "    def predict(self, word):\n",
    "        index = self.word2index[word]\n",
    "        return (self.U[index] + self.V[:, index]) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- epoch: 0 CE: 7.488471782588354------\n",
      "----- epoch: 1 CE: 7.487727317427425------\n",
      "----- epoch: 2 CE: 7.486729507498391------\n",
      "----- epoch: 3 CE: 7.485487658415663------\n",
      "----- epoch: 4 CE: 7.484029036849684------\n",
      "----- epoch: 5 CE: 7.482333583013913------\n",
      "----- epoch: 6 CE: 7.480427070295531------\n",
      "----- epoch: 7 CE: 7.478310816980172------\n",
      "----- epoch: 8 CE: 7.475939589838471------\n",
      "----- epoch: 9 CE: 7.473352285250231------\n",
      "----- epoch: 10 CE: 7.470537906979288------\n",
      "----- epoch: 11 CE: 7.467500785591814------\n",
      "----- epoch: 12 CE: 7.46422989741232------\n",
      "----- epoch: 13 CE: 7.460701751778257------\n",
      "----- epoch: 14 CE: 7.456941394527926------\n",
      "----- epoch: 15 CE: 7.452944273802208------\n",
      "----- epoch: 16 CE: 7.448661757567692------\n",
      "----- epoch: 17 CE: 7.444119136127311------\n",
      "----- epoch: 18 CE: 7.439303322536477------\n",
      "----- epoch: 19 CE: 7.434255498230487------\n",
      "----- epoch: 20 CE: 7.4288557172583385------\n",
      "----- epoch: 21 CE: 7.423246721048898------\n",
      "----- epoch: 22 CE: 7.417261667122427------\n",
      "----- epoch: 23 CE: 7.410950447462692------\n",
      "----- epoch: 24 CE: 7.40426057954689------\n",
      "----- epoch: 25 CE: 7.397205944770615------\n",
      "----- epoch: 26 CE: 7.389858743770043------\n",
      "----- epoch: 27 CE: 7.382126238256906------\n",
      "----- epoch: 28 CE: 7.373999205940228------\n",
      "----- epoch: 29 CE: 7.365353219984092------\n",
      "----- epoch: 30 CE: 7.356301518089936------\n",
      "----- epoch: 31 CE: 7.346625981829676------\n",
      "----- epoch: 32 CE: 7.336603723516958------\n",
      "----- epoch: 33 CE: 7.325937556918844------\n",
      "----- epoch: 34 CE: 7.314757487337073------\n",
      "----- epoch: 35 CE: 7.3028421574101285------\n",
      "----- epoch: 36 CE: 7.2904986994060055------\n",
      "----- epoch: 37 CE: 7.277238626505106------\n",
      "----- epoch: 38 CE: 7.263122374785203------\n",
      "----- epoch: 39 CE: 7.248549365520125------\n",
      "----- epoch: 40 CE: 7.232829738789422------\n",
      "----- epoch: 41 CE: 7.216583259290099------\n",
      "----- epoch: 42 CE: 7.199165404525149------\n",
      "----- epoch: 43 CE: 7.181014961439425------\n",
      "----- epoch: 44 CE: 7.161787805769219------\n",
      "----- epoch: 45 CE: 7.1417730747701915------\n",
      "----- epoch: 46 CE: 7.1207361621313385------\n",
      "----- epoch: 47 CE: 7.098766852296512------\n",
      "----- epoch: 48 CE: 7.07609179905285------\n",
      "----- epoch: 49 CE: 7.05293172189303------\n",
      "----- epoch: 50 CE: 7.029396363617312------\n",
      "----- epoch: 51 CE: 7.0058419011388136------\n",
      "----- epoch: 52 CE: 6.982288761104782------\n",
      "----- epoch: 53 CE: 6.9593292120271055------\n",
      "----- epoch: 54 CE: 6.9361935628644575------\n",
      "----- epoch: 55 CE: 6.913091728479952------\n",
      "----- epoch: 56 CE: 6.8901902781114845------\n",
      "----- epoch: 57 CE: 6.867261195922583------\n",
      "----- epoch: 58 CE: 6.844284521307607------\n",
      "----- epoch: 59 CE: 6.821444621186771------\n",
      "----- epoch: 60 CE: 6.7989888645382335------\n",
      "----- epoch: 61 CE: 6.775803436468075------\n",
      "----- epoch: 62 CE: 6.753862892348791------\n",
      "----- epoch: 63 CE: 6.730996677052391------\n",
      "----- epoch: 64 CE: 6.708211561269041------\n",
      "----- epoch: 65 CE: 6.685594498973389------\n",
      "----- epoch: 66 CE: 6.662985137736745------\n",
      "----- epoch: 67 CE: 6.640317757287976------\n",
      "----- epoch: 68 CE: 6.617068534863425------\n",
      "----- epoch: 69 CE: 6.594412081905172------\n",
      "----- epoch: 70 CE: 6.571439395028231------\n",
      "----- epoch: 71 CE: 6.548373499016479------\n",
      "----- epoch: 72 CE: 6.524822732367541------\n",
      "----- epoch: 73 CE: 6.501567589361901------\n",
      "----- epoch: 74 CE: 6.477787289503177------\n",
      "----- epoch: 75 CE: 6.45394082692833------\n",
      "----- epoch: 76 CE: 6.429527394780592------\n",
      "----- epoch: 77 CE: 6.405209915942153------\n",
      "----- epoch: 78 CE: 6.380244608815736------\n",
      "----- epoch: 79 CE: 6.355791394803497------\n",
      "----- epoch: 80 CE: 6.3303803609238765------\n",
      "----- epoch: 81 CE: 6.304777680869499------\n",
      "----- epoch: 82 CE: 6.279459573312432------\n",
      "----- epoch: 83 CE: 6.25357150570558------\n",
      "----- epoch: 84 CE: 6.226877645893853------\n",
      "----- epoch: 85 CE: 6.20059702047768------\n",
      "----- epoch: 86 CE: 6.174056219603553------\n",
      "----- epoch: 87 CE: 6.147252602270918------\n",
      "----- epoch: 88 CE: 6.120028239168763------\n",
      "----- epoch: 89 CE: 6.092734062774088------\n",
      "----- epoch: 90 CE: 6.0653322458672845------\n",
      "----- epoch: 91 CE: 6.03757302174676------\n",
      "----- epoch: 92 CE: 6.009689666910561------\n",
      "----- epoch: 93 CE: 5.981615783165851------\n",
      "----- epoch: 94 CE: 5.953362922109056------\n",
      "----- epoch: 95 CE: 5.925560645679923------\n",
      "----- epoch: 96 CE: 5.897247380376343------\n",
      "----- epoch: 97 CE: 5.8686631588428755------\n",
      "----- epoch: 98 CE: 5.840013985665587------\n",
      "----- epoch: 99 CE: 5.811359754940619------\n",
      "----- epoch: 100 CE: 5.782661351148075------\n",
      "----- epoch: 101 CE: 5.753920811075637------\n",
      "----- epoch: 102 CE: 5.725211081016009------\n",
      "----- epoch: 103 CE: 5.696735302058469------\n",
      "----- epoch: 104 CE: 5.667434320025036------\n",
      "----- epoch: 105 CE: 5.638470232305498------\n",
      "----- epoch: 106 CE: 5.609993362745029------\n",
      "----- epoch: 107 CE: 5.5806132432292594------\n",
      "----- epoch: 108 CE: 5.551300893299521------\n",
      "----- epoch: 109 CE: 5.522481627543683------\n",
      "----- epoch: 110 CE: 5.493215512416393------\n",
      "----- epoch: 111 CE: 5.464159516897422------\n",
      "----- epoch: 112 CE: 5.4347457323397474------\n",
      "----- epoch: 113 CE: 5.405339420981009------\n",
      "----- epoch: 114 CE: 5.375569407832212------\n",
      "----- epoch: 115 CE: 5.346281823913548------\n",
      "----- epoch: 116 CE: 5.316488715322918------\n",
      "----- epoch: 117 CE: 5.287020582120162------\n",
      "----- epoch: 118 CE: 5.256863656747341------\n",
      "----- epoch: 119 CE: 5.226869582287495------\n",
      "----- epoch: 120 CE: 5.196517652337709------\n",
      "----- epoch: 121 CE: 5.166827240923416------\n",
      "----- epoch: 122 CE: 5.136335643288438------\n",
      "----- epoch: 123 CE: 5.106370177870177------\n",
      "----- epoch: 124 CE: 5.0756507225434895------\n",
      "----- epoch: 125 CE: 5.045454151698256------\n",
      "----- epoch: 126 CE: 5.014201717037362------\n",
      "----- epoch: 127 CE: 4.9836883525947036------\n",
      "----- epoch: 128 CE: 4.9524990062516805------\n",
      "----- epoch: 129 CE: 4.921756483903656------\n",
      "----- epoch: 130 CE: 4.890952586208395------\n",
      "----- epoch: 131 CE: 4.859650349681037------\n",
      "----- epoch: 132 CE: 4.828094484676665------\n",
      "----- epoch: 133 CE: 4.796504901024014------\n",
      "----- epoch: 134 CE: 4.765106150190959------\n",
      "----- epoch: 135 CE: 4.73376319306064------\n",
      "----- epoch: 136 CE: 4.702187378050567------\n",
      "----- epoch: 137 CE: 4.670339781946872------\n",
      "----- epoch: 138 CE: 4.638797841451953------\n",
      "----- epoch: 139 CE: 4.607622911512099------\n",
      "----- epoch: 140 CE: 4.5752531645748356------\n",
      "----- epoch: 141 CE: 4.5430783940778126------\n",
      "----- epoch: 142 CE: 4.5113113012925075------\n",
      "----- epoch: 143 CE: 4.479126085034928------\n",
      "----- epoch: 144 CE: 4.447041343371868------\n",
      "----- epoch: 145 CE: 4.415165499351658------\n",
      "----- epoch: 146 CE: 4.382907463999599------\n",
      "----- epoch: 147 CE: 4.350329874442648------\n",
      "----- epoch: 148 CE: 4.318145191315926------\n",
      "----- epoch: 149 CE: 4.286105807539214------\n",
      "----- epoch: 150 CE: 4.254164551844783------\n",
      "----- epoch: 151 CE: 4.221740393928579------\n",
      "----- epoch: 152 CE: 4.189321107688494------\n",
      "----- epoch: 153 CE: 4.157756811154738------\n",
      "----- epoch: 154 CE: 4.12480395056379------\n",
      "----- epoch: 155 CE: 4.092246021322374------\n",
      "----- epoch: 156 CE: 4.060083297085058------\n",
      "----- epoch: 157 CE: 4.028041759178316------\n",
      "----- epoch: 158 CE: 3.9955837848071094------\n",
      "----- epoch: 159 CE: 3.963129123466462------\n",
      "----- epoch: 160 CE: 3.9307908973348322------\n",
      "----- epoch: 161 CE: 3.8989613564088015------\n",
      "----- epoch: 162 CE: 3.8665933518260687------\n",
      "----- epoch: 163 CE: 3.834012972786455------\n",
      "----- epoch: 164 CE: 3.802415448010592------\n",
      "----- epoch: 165 CE: 3.7701616764615453------\n",
      "----- epoch: 166 CE: 3.7381693093331023------\n",
      "----- epoch: 167 CE: 3.7064150310708612------\n",
      "----- epoch: 168 CE: 3.673943650309409------\n",
      "----- epoch: 169 CE: 3.6424193941657794------\n",
      "----- epoch: 170 CE: 3.6101513884292604------\n",
      "----- epoch: 171 CE: 3.579111366330292------\n",
      "----- epoch: 172 CE: 3.5469097089297428------\n",
      "----- epoch: 173 CE: 3.515538328307452------\n",
      "----- epoch: 174 CE: 3.4832263129449452------\n",
      "----- epoch: 175 CE: 3.4529020958022194------\n",
      "----- epoch: 176 CE: 3.4210834394703897------\n",
      "----- epoch: 177 CE: 3.3894335660886954------\n",
      "----- epoch: 178 CE: 3.3584706444564514------\n",
      "----- epoch: 179 CE: 3.327045728383389------\n",
      "----- epoch: 180 CE: 3.296870571960877------\n",
      "----- epoch: 181 CE: 3.2656121863926786------\n",
      "----- epoch: 182 CE: 3.235105624412562------\n",
      "----- epoch: 183 CE: 3.2045182729661037------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- epoch: 184 CE: 3.173442131691417------\n",
      "----- epoch: 185 CE: 3.1434159350159563------\n",
      "----- epoch: 186 CE: 3.113183893721283------\n",
      "----- epoch: 187 CE: 3.0826206519816632------\n",
      "----- epoch: 188 CE: 3.0517360130886444------\n",
      "----- epoch: 189 CE: 3.0229620834946043------\n",
      "----- epoch: 190 CE: 2.992461927524104------\n",
      "----- epoch: 191 CE: 2.9633297423400533------\n",
      "----- epoch: 192 CE: 2.93402718411701------\n",
      "----- epoch: 193 CE: 2.9039954597590905------\n",
      "----- epoch: 194 CE: 2.8749616098855877------\n",
      "----- epoch: 195 CE: 2.8449587273446575------\n",
      "----- epoch: 196 CE: 2.8162566495180315------\n",
      "----- epoch: 197 CE: 2.788116239980124------\n",
      "----- epoch: 198 CE: 2.758896190766958------\n",
      "----- epoch: 199 CE: 2.7295897478310494------\n",
      "----- epoch: 200 CE: 2.7015182746458772------\n",
      "----- epoch: 201 CE: 2.673561733417366------\n",
      "----- epoch: 202 CE: 2.6454415525800377------\n",
      "----- epoch: 203 CE: 2.6169062285------\n",
      "----- epoch: 204 CE: 2.588456446340016------\n",
      "----- epoch: 205 CE: 2.560524090889976------\n",
      "----- epoch: 206 CE: 2.533001852736576------\n",
      "----- epoch: 207 CE: 2.5056358119774456------\n",
      "----- epoch: 208 CE: 2.4780119946156------\n",
      "----- epoch: 209 CE: 2.4507848593701387------\n",
      "----- epoch: 210 CE: 2.4238265501925964------\n",
      "----- epoch: 211 CE: 2.396467894531644------\n",
      "----- epoch: 212 CE: 2.369340033729105------\n",
      "----- epoch: 213 CE: 2.343242940737848------\n",
      "----- epoch: 214 CE: 2.3170778193776647------\n",
      "----- epoch: 215 CE: 2.28952961742217------\n",
      "----- epoch: 216 CE: 2.2639625378604906------\n",
      "----- epoch: 217 CE: 2.2381096135228775------\n",
      "----- epoch: 218 CE: 2.2127833981572462------\n",
      "----- epoch: 219 CE: 2.1862899719611413------\n",
      "----- epoch: 220 CE: 2.1614105657958538------\n",
      "----- epoch: 221 CE: 2.1352796656964634------\n",
      "----- epoch: 222 CE: 2.111217412899848------\n",
      "----- epoch: 223 CE: 2.086196531338339------\n",
      "----- epoch: 224 CE: 2.0612017928308783------\n",
      "----- epoch: 225 CE: 2.0362165591633494------\n",
      "----- epoch: 226 CE: 2.012386368235254------\n",
      "----- epoch: 227 CE: 1.988127290680632------\n",
      "----- epoch: 228 CE: 1.9642512719749117------\n",
      "----- epoch: 229 CE: 1.9404166816764399------\n",
      "----- epoch: 230 CE: 1.916987425065523------\n",
      "----- epoch: 231 CE: 1.893896027530966------\n",
      "----- epoch: 232 CE: 1.86973531744605------\n",
      "----- epoch: 233 CE: 1.8481939370338156------\n",
      "----- epoch: 234 CE: 1.8251373549998247------\n",
      "----- epoch: 235 CE: 1.8027096200679393------\n",
      "----- epoch: 236 CE: 1.7803237927964737------\n",
      "----- epoch: 237 CE: 1.7581636319370715------\n",
      "----- epoch: 238 CE: 1.736209093887442------\n",
      "----- epoch: 239 CE: 1.7148514831594774------\n",
      "----- epoch: 240 CE: 1.6933253173355953------\n",
      "----- epoch: 241 CE: 1.6721767531104745------\n",
      "----- epoch: 242 CE: 1.6513607461714017------\n",
      "----- epoch: 243 CE: 1.6308537953462614------\n",
      "----- epoch: 244 CE: 1.6106167227052117------\n",
      "----- epoch: 245 CE: 1.5899304773359653------\n",
      "----- epoch: 246 CE: 1.5696962079886072------\n",
      "----- epoch: 247 CE: 1.5508511036032084------\n",
      "----- epoch: 248 CE: 1.530396836249228------\n",
      "----- epoch: 249 CE: 1.5114292395449473------\n",
      "----- epoch: 250 CE: 1.4923882421801158------\n",
      "----- epoch: 251 CE: 1.4737703675714298------\n",
      "----- epoch: 252 CE: 1.455266772226017------\n",
      "----- epoch: 253 CE: 1.4373905530556657------\n",
      "----- epoch: 254 CE: 1.417734487881224------\n",
      "----- epoch: 255 CE: 1.4014381930776119------\n",
      "----- epoch: 256 CE: 1.3840264109808218------\n",
      "----- epoch: 257 CE: 1.366630119749912------\n",
      "----- epoch: 258 CE: 1.3492822130350257------\n",
      "----- epoch: 259 CE: 1.3318030722366216------\n",
      "----- epoch: 260 CE: 1.3163760750659714------\n",
      "----- epoch: 261 CE: 1.2997492980235577------\n",
      "----- epoch: 262 CE: 1.283496730072935------\n",
      "----- epoch: 263 CE: 1.2685210815518322------\n",
      "----- epoch: 264 CE: 1.2512492816290504------\n",
      "----- epoch: 265 CE: 1.2377815174176137------\n",
      "----- epoch: 266 CE: 1.2219178293419712------\n",
      "----- epoch: 267 CE: 1.2080544556658235------\n",
      "----- epoch: 268 CE: 1.1926656473819233------\n",
      "----- epoch: 269 CE: 1.178303403113753------\n",
      "----- epoch: 270 CE: 1.1645235720729172------\n",
      "----- epoch: 271 CE: 1.1508210510039218------\n",
      "----- epoch: 272 CE: 1.137026496459391------\n",
      "----- epoch: 273 CE: 1.124029596185453------\n",
      "----- epoch: 274 CE: 1.1104092308477203------\n",
      "----- epoch: 275 CE: 1.09853137433414------\n",
      "----- epoch: 276 CE: 1.0849516908114116------\n",
      "----- epoch: 277 CE: 1.0725559515068255------\n",
      "----- epoch: 278 CE: 1.0604457543628114------\n",
      "----- epoch: 279 CE: 1.0489116540437013------\n",
      "----- epoch: 280 CE: 1.0369773716305788------\n",
      "----- epoch: 281 CE: 1.0253055359173535------\n",
      "----- epoch: 282 CE: 1.0143658391625043------\n",
      "----- epoch: 283 CE: 1.0030026992397032------\n",
      "----- epoch: 284 CE: 0.9927891384682306------\n",
      "----- epoch: 285 CE: 0.9815318753276138------\n",
      "----- epoch: 286 CE: 0.9707968007694204------\n",
      "----- epoch: 287 CE: 0.9604886072738081------\n",
      "----- epoch: 288 CE: 0.9508353369462719------\n",
      "----- epoch: 289 CE: 0.9403775326365914------\n",
      "----- epoch: 290 CE: 0.9306612525582979------\n",
      "----- epoch: 291 CE: 0.9215185958385433------\n",
      "----- epoch: 292 CE: 0.9122368686725401------\n",
      "----- epoch: 293 CE: 0.9022716229861091------\n",
      "----- epoch: 294 CE: 0.8936444655301298------\n",
      "----- epoch: 295 CE: 0.8841676737238416------\n",
      "----- epoch: 296 CE: 0.8760871390491797------\n",
      "----- epoch: 297 CE: 0.8672823936344495------\n",
      "----- epoch: 298 CE: 0.8587090529157371------\n",
      "----- epoch: 299 CE: 0.8516052572864279------\n",
      "----- epoch: 300 CE: 0.8433029367237784------\n",
      "----- epoch: 301 CE: 0.8360589558756992------\n",
      "----- epoch: 302 CE: 0.8283332535444542------\n",
      "----- epoch: 303 CE: 0.8206812996539568------\n",
      "----- epoch: 304 CE: 0.8139591650778477------\n",
      "----- epoch: 305 CE: 0.8061886245880885------\n",
      "----- epoch: 306 CE: 0.7983864912325356------\n",
      "----- epoch: 307 CE: 0.7918819490331531------\n",
      "----- epoch: 308 CE: 0.7848028111393385------\n",
      "----- epoch: 309 CE: 0.7792253417886233------\n",
      "----- epoch: 310 CE: 0.7717924235225042------\n",
      "----- epoch: 311 CE: 0.7647519832511533------\n",
      "----- epoch: 312 CE: 0.7599672559542106------\n",
      "----- epoch: 313 CE: 0.7529710842229835------\n",
      "----- epoch: 314 CE: 0.7479956368561629------\n",
      "----- epoch: 315 CE: 0.7417041924369643------\n",
      "----- epoch: 316 CE: 0.7353515074975032------\n",
      "----- epoch: 317 CE: 0.7299907609125117------\n",
      "----- epoch: 318 CE: 0.7235100965691138------\n",
      "----- epoch: 319 CE: 0.7187228933910772------\n",
      "----- epoch: 320 CE: 0.7134212471929683------\n",
      "----- epoch: 321 CE: 0.7086468536902121------\n",
      "----- epoch: 322 CE: 0.7038930236705044------\n",
      "----- epoch: 323 CE: 0.6984374611088651------\n",
      "----- epoch: 324 CE: 0.6936587536392853------\n",
      "----- epoch: 325 CE: 0.6881826417944802------\n",
      "----- epoch: 326 CE: 0.6833250458608304------\n",
      "----- epoch: 327 CE: 0.6798447917987054------\n",
      "----- epoch: 328 CE: 0.6750890467870334------\n",
      "----- epoch: 329 CE: 0.6695827676742491------\n",
      "----- epoch: 330 CE: 0.6661110833883058------\n",
      "----- epoch: 331 CE: 0.6607548575536729------\n",
      "----- epoch: 332 CE: 0.6568764946438723------\n",
      "----- epoch: 333 CE: 0.6531124730081894------\n",
      "----- epoch: 334 CE: 0.6487138187503553------\n",
      "----- epoch: 335 CE: 0.6450282685403694------\n",
      "----- epoch: 336 CE: 0.6412457996249681------\n",
      "----- epoch: 337 CE: 0.637361087196691------\n",
      "----- epoch: 338 CE: 0.6333892257451229------\n",
      "----- epoch: 339 CE: 0.6299558045895083------\n",
      "----- epoch: 340 CE: 0.6254563299105237------\n",
      "----- epoch: 341 CE: 0.6227021619633524------\n",
      "----- epoch: 342 CE: 0.6186147954448616------\n",
      "----- epoch: 343 CE: 0.6156005288327583------\n",
      "----- epoch: 344 CE: 0.611984747414226------\n",
      "----- epoch: 345 CE: 0.6093271875990773------\n",
      "----- epoch: 346 CE: 0.6066769547009945------\n",
      "----- epoch: 347 CE: 0.6028383540221074------\n",
      "----- epoch: 348 CE: 0.5996001543191393------\n",
      "----- epoch: 349 CE: 0.5968916822076149------\n",
      "----- epoch: 350 CE: 0.5944173521577188------\n",
      "----- epoch: 351 CE: 0.5913359227318532------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-ffeb9514b7c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCBOW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-37-58d73cec54e8>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0mtrain_right\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_right_X\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0mtrain_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_Y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                 \u001b[0mavg_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_average_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_right\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-58d73cec54e8>\u001b[0m in \u001b[0;36mget_average_context\u001b[0;34m(self, left_context, right_context)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \"\"\"\n\u001b[1;32m     64\u001b[0m         \u001b[0mleft_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mall_onehot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mleft_context\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector_representation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_onehot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-58d73cec54e8>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \"\"\"\n\u001b[1;32m     64\u001b[0m         \u001b[0mleft_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mall_onehot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mleft_context\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector_representation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_onehot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-58d73cec54e8>\u001b[0m in \u001b[0;36mone_hot_vector\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mone\u001b[0m \u001b[0mhot\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \"\"\"\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "f = open('./data.txt')\n",
    "a = CBOW(f.readline(), 2, 100)\n",
    "a.fit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latter\n",
      "sustained\n",
      "supply\n",
      "whole\n",
      "centre\n",
      "dollar\n"
     ]
    }
   ],
   "source": [
    "similarity = []\n",
    "ground = a.predict('dollar')\n",
    "for word in a.vocab:\n",
    "    similarity.append(cosineSimilarity(ground, a.predict(word)))\n",
    "index = np.argsort(similarity)[-6:]\n",
    "for i in index:\n",
    "    print(a.vocab[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ratification\n",
      "watch\n",
      "fiveweek\n",
      "south\n",
      "global\n",
      "stock\n"
     ]
    }
   ],
   "source": [
    "similarity = []\n",
    "ground = a.predict('stock')\n",
    "for word in a.vocab:\n",
    "    similarity.append(cosineSimilarity(ground, a.predict(word)))\n",
    "index = np.argsort(similarity)[-6:]\n",
    "for i in index:\n",
    "    print(a.vocab[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "china\n",
      "strategic\n",
      "leaders\n",
      "economics\n",
      "commerce\n",
      "mortgage\n"
     ]
    }
   ],
   "source": [
    "similarity = []\n",
    "ground = a.predict('mortgage')\n",
    "for word in a.vocab:\n",
    "    similarity.append(cosineSimilarity(ground, a.predict(word)))\n",
    "index = np.argsort(similarity)[-6:]\n",
    "for i in index:\n",
    "    print(a.vocab[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = copy.deepcopy(a.U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = copy.deepcopy(a.V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
