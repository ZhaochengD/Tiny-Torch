{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from MLP import *\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Constants:\n",
    "    \"\"\"\n",
    "    Recommended hyperparameters.\n",
    "    Feel free to add/remove/modify these values to run your code.\n",
    "    However, keep a copy of the original values as they will be used as\n",
    "    reference for the local and autograder tests.\n",
    "\n",
    "    In order to access any hyperparameter from the Constants class, just call Constants.xxx.\n",
    "    \"\"\"\n",
    "    num_epochs = 20 # number of epochs\n",
    "    num_iterate = 100 # number of iterations per epoch.\n",
    "    batch_size = 100\n",
    "    learning_rate = 0.00005\n",
    "    n_critic = 5\n",
    "    c = 0.01\n",
    "    beta = 0.9 # RMSProp Parameter\n",
    "    latent= 100 # latent space dimension\n",
    "    discriminator_hidden_sizes = [784, 512, 256, 1]\n",
    "    generator_hidden_sizes = [latent, 256, 512, 1024, 784]\n",
    "    eps = 1e-8\n",
    "\n",
    "def weights_init(inp_dim, out_dim):\n",
    "    \"\"\"\n",
    "    Function for weights initialization\n",
    "    :param inp_dim: Input dimension of the weight matrix\n",
    "    :param out_dim: Output dimension of the weight matrix\n",
    "    \"\"\"\n",
    "    b = np.sqrt(6)/np.sqrt(inp_dim+out_dim)\n",
    "    return np.random.uniform(-b,b,(inp_dim,out_dim))\n",
    "\n",
    "def biases_init(dim):\n",
    "    \"\"\"\n",
    "    Function for biases initialization\n",
    "    :param dim: Dimension of the biases vector\n",
    "    \"\"\"\n",
    "    return np.zeros((1, dim)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize your weights, biases and anything you need here.\n",
    "        Please follow this naming convention for your variables (helpful for tests.py and autograder)\n",
    "            Variable name for weight matrices: W0, W1, ...\n",
    "            Variable name for biases: b0, b1, ...\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dlayer = Constants.discriminator_hidden_sizes\n",
    "        self.nlayer = len(self.dlayer) - 1\n",
    "        self.linear_layers = [Linear(self.dlayer[i], self.dlayer[i+1], \n",
    "                                     weights_init, biases_init) for i in range(len(self.dlayer) - 1)]\n",
    "        self.beta = Constants.beta\n",
    "        self.lr = Constants.learning_rate\n",
    "        self.c = Constants.c\n",
    "        \n",
    "        self.activations = [LRelu('wgan'), LRelu('wgan')]\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for discriminator\n",
    "        :param x: Input for the forward pass with shape (batch_size, 28, 28, 1)\n",
    "        :return Output of the discriminator with shape (batch_size, 1)\n",
    "        NOTE: Given you are using linear layers, you will have to appropriately reshape your data.\n",
    "        \"\"\"\n",
    "        for LayderIdx in range(self.nlayer):\n",
    "            x = self.linear_layers[LayderIdx].forward(x)\n",
    "            if LayderIdx != self.nlayer - 1:\n",
    "                x = self.activations[LayderIdx].forward(x)\n",
    "        self.output = x\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, logit, image_type, update_params=True):\n",
    "        \"\"\"\n",
    "        Backward pass for discriminator. \n",
    "        Implement RMSProp for the gradient update.\n",
    "        Use/set the Learning Rate in the Constants class.\n",
    "\n",
    "        :param logit: logit value with shape (batch_size, 1)\n",
    "        :param inp: input image with shape (batch_size, 28, 28, 1). This parameter might not be required, \n",
    "        depending on your implementation.\n",
    "        :image_type: Integer value -1 or 1 depending on whether it is a real or fake image.\n",
    "            image_type will ensure that the gradients are taken in the rightion.\n",
    "        \"\"\"\n",
    "        d = image_type * np.ones_like(logit)\n",
    "        for LayderIdx in range(self.nlayer-1, -1, -1):\n",
    "            if LayderIdx != self.nlayer-1:\n",
    "                d = self.activations[LayderIdx].backward(copy.deepcopy(d))\n",
    "            d = self.linear_layers[LayderIdx].backward(copy.deepcopy(d))\n",
    "        return d\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for linear in self.linear_layers:\n",
    "            linear.dW.fill(0.0)\n",
    "            linear.db.fill(0.0)\n",
    "    \n",
    "    def step(self):\n",
    "        for i in range(len(self.linear_layers)):\n",
    "            self.linear_layers[i].rmsprop_W = self.beta * self.linear_layers[i].rmsprop_W + (1 - self.beta) * np.square(self.linear_layers[i].dW)\n",
    "            self.linear_layers[i].W += (self.lr * self.linear_layers[i].dW) / (np.sqrt(self.linear_layers[i].rmsprop_W) + Constants.eps)\n",
    "            \n",
    "            self.linear_layers[i].rmsprop_b = self.beta * self.linear_layers[i].rmsprop_b + (1 - self.beta) * self.linear_layers[i].db ** 2\n",
    "            self.linear_layers[i].b += (self.lr * self.linear_layers[i].db) / (np.sqrt(self.linear_layers[i].rmsprop_b) + Constants.eps)                          \n",
    "\n",
    "    def weight_clipping(self):\n",
    "                                               \n",
    "        \"\"\"\n",
    "        Implement weight clipping for discriminator's weights and biases.\n",
    "        Set/use the value defined in the Constants class.\n",
    "        \"\"\"\n",
    "        for i in range(len(self.linear_layers)):\n",
    "            self.linear_layers[i].W = np.clip(self.linear_layers[i].W, - self.c, + self.c)\n",
    "            self.linear_layers[i].b = np.clip(self.linear_layers[i].b, - self.c, + self.c)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Do not change/remove.\n",
    "        \"\"\"\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize your weights, biases and anything you need here.\n",
    "        Please follow this naming convention for your variables (helpful for tests.py and autograder)\n",
    "            Variable name for weight matrices: W0, W1, ...\n",
    "            Variable name for biases: b0, b1, ...\n",
    "        \"\"\"\n",
    "        self.dlayer = Constants.generator_hidden_sizes\n",
    "        self.nlayer = len(self.dlayer) - 1\n",
    "        self.linear_layers = [Linear(self.dlayer[i], self.dlayer[i+1], \n",
    "                                     weights_init, biases_init) for i in range(len(self.dlayer) - 1)]\n",
    "        self.activations = [LRelu('wgan'), LRelu('wgan'), LRelu('wgan'), Tanh()]\n",
    "        \n",
    "        self.beta = Constants.beta\n",
    "        self.lr = Constants.learning_rate\n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Forward pass for generator\n",
    "        :param z: Input for the forward pass with shape (batch_size, 100)\n",
    "                 Output of the final linear layer will be of shape (batch_size, 784)\n",
    "        :return Output of the generator with shape (batch_size, 28, 28, 1)\n",
    "        \"\"\"\n",
    "        for LayderIdx in range(self.nlayer):\n",
    "            z = self.linear_layers[LayderIdx].forward(z)\n",
    "            z = self.activations[LayderIdx].forward(z)      \n",
    "        self.output = z\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, fake_logit, discriminator):\n",
    "        \"\"\"\n",
    "        Backward pass for generator\n",
    "        Implement RMSProp for the gradient update.\n",
    "        Use/set the Learning Rate in the Constants class.\n",
    "\n",
    "        :param fake_logit: Logit output from the discriminator with shape (batch_size, 1)\n",
    "        :param fake_input: Fake images generated by the generator with shape (batch_size, 28, 28, 1) \n",
    "            -- may or may not be required depending upon your implementation.\n",
    "        :param discriminator: discriminator object\n",
    "        NOTE: In order to perform backward, you may or may not (based on your \n",
    "        implementation) need to call the backward function in the discriminator.\n",
    "              In such an event, make sure that the Discriminator weights are *not* \n",
    "              being updated for this particular call.\n",
    "        \"\"\"\n",
    "        d = discriminator.backward(fake_logit, 1)\n",
    "        for LayderIdx in range(self.nlayer-1, -1, -1):\n",
    "            d = self.activations[LayderIdx].backward(d)\n",
    "            d = self.linear_layers[LayderIdx].backward(copy.deepcopy(d))\n",
    "        return d\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for linear in self.linear_layers:\n",
    "            linear.dW.fill(0.0)\n",
    "            linear.db.fill(0.0)\n",
    "    \n",
    "    def step(self):\n",
    "        for i in range(len(self.linear_layers)):\n",
    "            self.linear_layers[i].rmsprop_W = (self.beta * self.linear_layers[i].rmsprop_W + \n",
    "                                               (1 - self.beta) * self.linear_layers[i].dW ** 2)\n",
    "            self.linear_layers[i].W += (self.lr * self.linear_layers[i].dW / \n",
    "                                        (np.sqrt(self.linear_layers[i].rmsprop_W) + Constants.eps))\n",
    "            \n",
    "            self.linear_layers[i].rmsprop_b = (self.beta * self.linear_layers[i].rmsprop_b + \n",
    "                                               (1 - self.beta) * self.linear_layers[i].db ** 2)\n",
    "            self.linear_layers[i].b += (self.lr * self.linear_layers[i].db / \n",
    "                                        (np.sqrt(self.linear_layers[i].rmsprop_b) + Constants.eps))  \n",
    "    \n",
    "    def __call__(self, z):\n",
    "        \"\"\"\n",
    "        Do not change/remove.\n",
    "        \"\"\"\n",
    "        return self.forward(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGAN(object):\n",
    "\n",
    "    def __init__(self, generator, discriminator):\n",
    "        \"\"\"\n",
    "        Initialize the GAN with your discriminator, generator and anything you need here.\n",
    "        Feel free to change/modify your function signatures here, this will *not* be autograded.\n",
    "        \"\"\"\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.n_critic = Constants.n_critic\n",
    "        self.batch_size = Constants.batch_size\n",
    "        self.seed = 19980709\n",
    "\n",
    "    def linear_interpolation(self):\n",
    "        \"\"\"\n",
    "        Generate linear interpolation between two data points. \n",
    "        (See example on Lec15 slides page 29\n",
    "        and feel free to modify the function signature. This function is not graded by autograder.)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def train_one_batch(self, train_X):\n",
    "        \"\"\"\n",
    "        Determine how to train the WGAN. \n",
    "        You can use the \"img_tile\" functions in utils.py to visualize your generated batch.\n",
    "\n",
    "        If you make changes to the vizualization procedure for the fake batches, \n",
    "        please include them in your utils.py file.\n",
    "        \"\"\"\n",
    "        for i in range(self.n_critic):\n",
    "            idx = np.random.choice(np.arange(len(train_X)), self.batch_size, replace=True)\n",
    "            batch_real = train_X[idx]\n",
    "            batch_noise = np.random.randn(self.batch_size, 100)\n",
    "            batch_fake = self.generator(batch_noise)\n",
    "            \n",
    "            self.discriminator.zero_grad()\n",
    "            \n",
    "            score_real = self.discriminator(batch_real)\n",
    "            self.discriminator.backward(score_real, 1)\n",
    "            score_fake = self.discriminator(batch_fake)\n",
    "            self.discriminator.backward(score_fake, -1)\n",
    "            \n",
    "            self.discriminator.step()\n",
    "            self.discriminator.weight_clipping()\n",
    "        \n",
    "        batch_noise = np.random.randn(self.batch_size, 100)\n",
    "        batch_fake = self.generator(batch_noise)\n",
    "        \n",
    "        self.generator.zero_grad()\n",
    "        fake_score = self.discriminator(batch_fake)\n",
    "        self.generator.backward(fake_score, self.discriminator)\n",
    "        self.generator.step()\n",
    "    \n",
    "    def generate(self, batch_size):\n",
    "        \n",
    "        batch_noise = np.random.randn(batch_size, 100)\n",
    "        batch_fake = self.generator(batch_noise)\n",
    "        return batch_fake.reshape(28, 28, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- epoch0 -----\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \"\"\"\n",
    "    You can implement your training and testing loop here.\n",
    "    You MUST use your class implementations to train the model and to get the results.\n",
    "    DO NOT use pytorch or tensorflow get the results. The results generated using these\n",
    "    libraries will be different as compared to your implementation.\n",
    "    \"\"\"\n",
    "    train_X, _, _ = mnist_reader([i for i in range(10)])\n",
    "    np.random.shuffle(train_X)\n",
    "    \n",
    "    model = WGAN(Generator(), Discriminator())\n",
    "    for ep in range(1): #Constants.num_epochs\n",
    "        print('----- epoch{} -----'.format(ep))\n",
    "        for it in range(Constants.num_iterate):\n",
    "            model.train_one_batch(train_X.reshape(train_X.shape[0], -1))\n",
    "        result = model.generate(1)\n",
    "        plt.imshow(result)\n",
    "        plt.show()\n",
    "        # img_tile(result, './gen_img/', ep, 1, aspect_ratio=1.0, border=1, border_color=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
